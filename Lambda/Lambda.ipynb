{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01912d6",
   "metadata": {},
   "source": [
    "# Lambda\n",
    "\n",
    "Lambda is a serverless computing service from AWS. This means that we can lauch code without worrying about provisioning the necessary infrastructure. \n",
    "\n",
    "# Why is Lambda Awesome ?\n",
    "\n",
    "**It allows us to focus on the important part**\n",
    "\n",
    "Normally, whenever we want to execute an script, we need a computer that runs it. If we want to run this script remotely, we then need a server (something like an EC2 machine). However, before we can run the code, we have to set up the server, configure the environment, secure the system and so on. If we want to change our code, then we have to access the server, change the code, make sure that everything still runs... it can quickly become a lot of work.\n",
    "\n",
    "While there are tools in the market that help us in this process, AWS Lambda allows us to focus on the most important part, which is **write the code**.\n",
    "\n",
    "**Very little overhead configuration (can be) needed**\n",
    "\n",
    "Configuring Lambda can become tricky, however compared with other services it's much simpler to set up and run. In case of AWS Glue, for example, the amount of configuration necessary to run it can be overwhelming, with lambda we can simply deploy code. Of course, on the long term for bigger projects this initial configuration can pay off (instead of running multiple lambda functions for every task), but for smaller tasks lambda can tackle the job very well.\n",
    "\n",
    "# Hands-On\n",
    "\n",
    "Maybe the best way to show the power of lambda is demonstrate how it works.\n",
    "\n",
    "Basically, what we will do is take some CSV files stored in a S3 Bucket and load them into a relational database using lambda.\n",
    "\n",
    "# S3 (Data Source)\n",
    "\n",
    "The files that are located on the S3 bucket were downloaded from [Kaggle](https://www.kaggle.com/). They're [data from a Brazillian e-commerce business](https://www.kaggle.com/olistbr/brazilian-ecommerce).\n",
    "\n",
    "![olist](Lambda/lambda_olist.png)\n",
    "\n",
    "# Database (Data Target)\n",
    "\n",
    "In the relation database, I already created the tables to hold the data that we need. In a \"real world\" environment the database schema would have already been decided by the database administrator or the person leading the development process. \n",
    "\n",
    "For this example, I just went with the structure of the files themselves, as they are organised in a logical way according to the [kaggle page](https://www.kaggle.com/olistbr/brazilian-ecommerce) from which I downloaded it.\n",
    "\n",
    "![tables](Lambda/lambda_rds.png)\n",
    "\n",
    "# Setting Up Lambda\n",
    "\n",
    "Even though lambda allows us to quickly run some code, we can't get around some \"configuration\". We have to make sure that our code can access the AWS resources it needs to move data from one place to the other. In this case, we're looking for access to S3 and RDS.\n",
    "\n",
    "One easy way to do this is to create a user with the permissions to access the services. In a company though you might have to talk to the cloud administrator so that he can give you the necessary permissions. \n",
    "\n",
    "This should generate an AWS Secret Key and a Key ID that allows us to authenticate the code we're running. \n",
    "\n",
    "![iam](Lambda/lambda_iam.png)\n",
    "\n",
    "*Note: there are many ways to do this and eventually you might want to do this differently in a company or in a production environment*\n",
    "# Creating the Function\n",
    "\n",
    "I'll create a lambda function from a blank template. The function's name will be S3ToRDS and it'll run with python 3.7 as the programming language.\n",
    "\n",
    "![aws_lambda_config](Lambda/lambda_config.png)\n",
    "\n",
    "# Configuring the Function\n",
    "\n",
    "Once the function is created, we're sent to a new screen. Here we can pick events that will trigger this function and also a destination, that is the service that the function will connect with.\n",
    "\n",
    "Since for this example we're doing a \"one time load\", I won't spend more time on this. It's nice to know, however, that we can configure lambda functions to react to something that has happened in our network or our application, but also to simply execute at a given time every day much like a cron job in linux.\n",
    "\n",
    "![aws_lambda_head_page](Lambda/lambda_head.png)\n",
    "\n",
    "At the bottom of the screen, we find other functionalities to configure our lambda function as we please. One interesting one is the possibility to set the environment variables. That allows us to pass some parameters to our code without altering the code itself.\n",
    "\n",
    "Here, for the sake of this example, I'll put the credentials of the user we created for our lambda functions.\n",
    "\n",
    "*Note: is this the right thing to do? [Some people have some concerns about having sensitive information in lambda environments](https://lumigo.io/blog/aws-lambda-vs-ec2/). Lambda does share resources with other AWS users, so we have to count on AWS to guarantee the privacy of this information. If we want to take care of safety ourselves, we have to lauch an EC2 machine, but then we lose the benefits of the serverless architecture*.\n",
    "\n",
    "![lambda_env](Lambda/lambda_env.png)\n",
    "\n",
    "# The Lambda Script\n",
    "\n",
    "Finally, we get to the scripting part. \n",
    "Basically what our lambda function has to do is access an S3 bucket containing CSV files, read each one of them and upload them to the database.\n",
    "\n",
    "We can obtain this result with the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb708050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import os\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    # Connects to the S3 Resource\n",
    "    s3 = boto3.resource(\"s3\",\n",
    "                    aws_access_key_id = os.environ[\"aws_secret_key_id\"],\n",
    "                   aws_secret_access_key = os.environ[\"aws_secret_access_key\"],\n",
    "                   region_name = os.environ[\"aws_region\"])\n",
    "\n",
    "    # Finds the Bucket\n",
    "    bucket = s3.Bucket(\"olist-dataset\")\n",
    "\n",
    "    # Connects to the Database\n",
    "    \n",
    "    conn = psycopg2.connect(os.environ[\"db_conn_string\"])\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Loops through the bucket contents\n",
    "    for file in bucket.objects.all():\n",
    "        \n",
    "        print(f\"{file.key}\")\n",
    "        \n",
    "        bytes_file = io.BytesIO() # Instantiates bytes file\n",
    "        \n",
    "        file_name = file.key\n",
    "        \n",
    "        if file_name == \"product_category_name_translation.csv\":\n",
    "            \n",
    "            target_table_name = \"product_category_name_translation\"\n",
    "            \n",
    "        else:\n",
    "            target_table_name = file_name.replace(\"olist_\",\"\").replace(\"_dataset.csv\",\"\")\n",
    "        \n",
    "        print(target_table_name)\n",
    "        \n",
    "        download_start = pd.Timestamp(\"now\")\n",
    "        print(\"Starting Download\")\n",
    "        \n",
    "        bucket.download_fileobj(file_name, bytes_file)\n",
    "        \n",
    "        download_end = pd.Timestamp(\"now\")\n",
    "        print(\"Finished Downloading. Elapsed Time = \", download_end - download_start)\n",
    "        \n",
    "        csv_file = io.TextIOWrapper(bytes_file, encoding=\"utf8\")\n",
    "        csv_file.seek(0)\n",
    "            \n",
    "        query = sql.SQL(\"\"\"COPY {table_name}\n",
    "                        FROM STDIN\n",
    "                        WITH HEADER CSV ENCODING 'utf8'\"\"\").format(table_name= sql.Identifier(target_table_name))\n",
    "            \n",
    "        upload_start = pd.Timestamp(\"now\")\n",
    "        print(\"Starting upload\")\n",
    "        cur.copy_expert(query, csv_file)\n",
    "        upload_end = pd.Timestamp(\"now\")\n",
    "        print(\"Finished Upload. Elapsed Time = \", upload_end - upload_start)\n",
    "            \n",
    "        conn.commit()\n",
    "        print(\"Done With Table {}\".format(target_table_name))\n",
    "        \n",
    "        \n",
    "    return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps('Function executed successfully!!!')\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d0823d",
   "metadata": {},
   "source": [
    "Now the only thing left to do is encapsulate the file in the lambda handler function that is provided with lambda.\n",
    "\n",
    "![handler](Lambda/lambda_handler.png)\n",
    "\n",
    "After we've clicked deploy to save our changes, we just have to push the test button to run it. \n",
    "\n",
    "# Or Not...\n",
    "\n",
    "Not everything is great with lambda. Some packages (like pandas and psycopg2 - or even some custom packages that we need) are not available out of the box. In order to allows lambda to use those packages, we have to prepare a layer to the function.\n",
    "\n",
    "There are many tutorials in the internet on how to do it, for example [this one](https://www.gcptutorials.com/post/how-to-use-pandas-in-aws-lambda).\n",
    "\n",
    "Basically, we have to:\n",
    "\n",
    "1) Create a python virtual environment\n",
    "\n",
    "2) Install the packages in the virtual environment (in my case, the \"requirements.txt\" file has to be included) and prepare a zip file with all the libraries. \n",
    "\n",
    "3) Than we have to create a layer for the function and attach it to the lambda function we want to run.\n",
    "\n",
    "![layer](Lambda/lambda_layer.png)\n",
    "\n",
    "Once done, we'll see that the lambda runs on a layer.\n",
    "\n",
    "![layer_successful](Lambda/lambda_layer_successful.png)\n",
    "\n",
    "Now, the function will run to the end.\n",
    "\n",
    "![layer_result](Lambda/lambda_result.png)\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Lambda is a great AWS service. It's quite flexible (in spite of the necessity to add layers sometimes) and allows us to focus on writing the code, instead of provisioning servers, installing an IDE, creating an environment and so on...\n",
    "\n",
    "It's surely one of the most useful AWS services and surely one that's worth learning how to use!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
